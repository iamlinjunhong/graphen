NODE_ENV=development

# Server
PORT=3001
CORS_ORIGIN=http://localhost:5173
MAX_UPLOAD_SIZE=52428800
RATE_LIMIT_WINDOW_MS=60000
RATE_LIMIT_MAX=100
LOG_LEVEL=info
CHAT_DB_PATH=data/chat.db
CACHE_DIR=data/cache
CHUNK_SIZE=1500
CHUNK_OVERLAP=200
MAX_CHUNKS_PER_DOCUMENT=500
MAX_DOCUMENT_ESTIMATED_TOKENS=500000

# LLM Provider Selection (gemini, qwen, or openai)
LLM_PROVIDER=gemini

# Gemini Configuration
GEMINI_API_KEY=
GEMINI_BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai/
GEMINI_CHAT_MODEL=gemini-3-flash-preview
GEMINI_EMBEDDING_MODEL=text-embedding-004

# Qwen Configuration
QWEN_API_KEY=
QWEN_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1
QWEN_CHAT_MODEL=qwen-max
QWEN_EMBEDDING_MODEL=text-embedding-v3

# OpenAI Configuration
OPENAI_API_KEY=
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_CHAT_MODEL=gpt-4o
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# LLM â€” Embedding (optional, separate provider for embeddings)
# Leave empty to use the same provider as chat
EMBEDDING_API_KEY=
EMBEDDING_BASE_URL=
LLM_MAX_CONCURRENT=5
LLM_MAX_RETRIES=3
LLM_RETRY_DELAY_MS=1000
LLM_REQUESTS_PER_MINUTE=30
LLM_TIMEOUT_MS=120000
EMBEDDING_DIMENSIONS=1024

# Neo4j
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=
NEO4J_DATABASE=neo4j
